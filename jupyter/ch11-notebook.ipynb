{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13435,"databundleVersionId":331452,"sourceType":"competition"}],"dockerImageVersionId":30123,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 11장 항공 사진 내 선인장 식별 경진대회 환경 세팅된 노트북 양식","metadata":{"papermill":{"duration":0.01747,"end_time":"2021-07-31T06:57:35.274085","exception":false,"start_time":"2021-07-31T06:57:35.256615","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\n\nlabels = pd.read_csv('/kaggle/input/aerial-cactus-identification/train.csv')\nsubmission = pd.read_csv('/kaggle/input/aerial-cactus-identification/sample_submission.csv')\n#submission.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nmpl.rc('font', size=15)\nplt.figure(figsize=(7,7))\nlabel=['Has cactus','hasn\\'t cactus']\nplt.pie(labels['has_cactus'].value_counts(), labels=label, autopct='%0.1f%%')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 이미지 출력 483\nfrom zipfile import ZipFile\n\nwith ZipFile('/kaggle/input/aerial-cactus-identification/train.zip') as zipper:\n    zipper.extractall()\n\nwith ZipFile('/kaggle/input/aerial-cactus-identification/test.zip') as zipper:\n    zipper.extractall()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n# print(len(os.listdir('train/')))\n# print(len(os.listdir('test/')))\n\n# 이미지 출력해보기 485\n\nimport matplotlib.gridspec as gridspec\nimport cv2\n\nmpl.rc('font', size=7)\nplt.figure(figsize=(15,6))\ngrid = gridspec.GridSpec(2,6)\n\nlast_has_cactus_img_name = labels[labels['has_cactus']==1]['id'][-12:]\n\nfor idx, img_name in enumerate(last_has_cactus_img_name):\n    img_path = 'train/'+img_name\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    ax = plt.subplot(grid[idx])\n    ax.imshow(img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 선인장이 아닌 사진 출력하기\nplt.figure(figsize=(15,6))\ngrid = gridspec.GridSpec(2,6)\n\nlast_has_cactus_img_name = labels[labels['has_cactus']==0]['id'][-12:]\n\nfor idx, img_name in enumerate(last_has_cactus_img_name):\n    img_path = 'train/'+img_name\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # 주석처리 해보세요. 칼라 읽는 방법을 변경한 코드입니다.\n    ax = plt.subplot(grid[idx])\n    ax.imshow(img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 489\nimport torch\nimport random\nimport numpy as np\nimport os\n\nseed = 50\nos.environ['PYTHONHASHSEED'] = str(seed)\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.daterministic = True\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.enabled = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device=torch.device('cuda')\nelse:\n    device=torch.device('cpu')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 데이터 처리 494\nimport pandas as pd\n\nlabels = pd.read_csv('/kaggle/input/aerial-cactus-identification/train.csv')\nsubmission = pd.read_csv('/kaggle/input/aerial-cactus-identification/sample_submission.csv')\n\n\nfrom zipfile import ZipFile\n\nwith ZipFile('/kaggle/input/aerial-cactus-identification/train.zip') as zipper:\n    zipper.extractall()\n\nwith ZipFile('/kaggle/input/aerial-cactus-identification/test.zip') as zipper:\n    zipper.extractall()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#데이터셋 정의하기 495~\nimport cv2\nfrom torch.utils.data import Dataset\n\nclass ImageDataset(Dataset):\n    # 초기화 메소드\n    def __init__(self, df, img_dir='./', transform=None):\n        super().__init__()\n        self.df = df\n        self.img_dir = img_dir\n        self.transform = transform\n\n    # 데이터셋 크기 반환 메소드\n    def __len__(self):\n        return len(self.df)\n\n    # 인덱스에 해당하는 데이터 반환 메소드\n    def __getitem__(self, idx):\n        img_id = self.df.iloc[idx, 0]\n        img_path = self.img_dir + img_id\n        image=cv2.imgred(img_path)\n        image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        label = self.df.iloc[idx,1]\n\n\n        if self.transform is not None:\n            image = self.transform(image)\n            return image, label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 498\nfrom torchvision import transforms\ntransform = transform.ToTensor()\n\ndataset_train = ImageDataset(df = train, img_dir=\"train/\", transform = transform)\ndataset_valid = ImageDataset(df = valid, img_dir=\"train/\", transform = transform)\n\ndataset_train.__len__()\ndataset_valid.__len__()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 데이터 로더 만들기 499\n\nfrom torch.utils.data import DataLoader\nloader_train = DataLoader(dataset = dataset_train, batch_size=32, shuffle=True)\nloader_valid = DataLoader(dataset = dataset_valid, batch_size=32, shuffle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 모델 만들기 501\nimport torch.nn as nn # 신경망 모듈 사용하기\nimport torch.nn.functional as F # 함수. 대문자로 표시하겠습니다.\n\n# class\nclass Model(nn.Module): # 신경망 모델\n    # 초기화\n    def __init__(self):\n        super().__inif__() # nn.Model 초기화 메소드 호출\n        self.conv1 = nn.Conv2d(in_channels=3, out_chnnels=32, kernel_size=3, padding=2)\n        self.conv2 = nn.Conv2d(in_channels=3, out_chnnels=32, kernel_size=3, padding=2)\n        self.max_pool = nn.MaxPool2d(kernel_size=2)\n        self.avg_pool = nn.AvgPool2d(kernel_size=2)\n        self.fc = nn.Linear(in_features=64*4*4, out_features=2)\n\n    # 순전파 출력\n    def forward(self, x):\n        x = self.max_pool(F.relu(self.conv1(x)))\n        x = self.max_pool(F.relu(self.conv2(x)))\n        x = self.svg_pool(x)\n        x = x.view(-1, 64 * 4 * 4)\n        x = self.fc(x)\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 490\nimport torch\nimport random\nimport numpy as np\nimport os\n\nseed = 50\nos.environ['PYTHONHASHSEED'] = str(seed)\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.enabled = False\n\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\ndevice\n\n\n# 데이터 처리 494\nimport pandas as pd\n\nlabels = pd.read_csv('/kaggle/input/aerial-cactus-identification/train.csv')\nsubmission = pd.read_csv('/kaggle/input/aerial-cactus-identification/sample_submission.csv')\n\n\nfrom zipfile import ZipFile\n\nwith ZipFile('/kaggle/input/aerial-cactus-identification/train.zip') as zipper:\n    zipper.extractall()\n\nwith ZipFile('/kaggle/input/aerial-cactus-identification/test.zip') as zipper:\n    zipper.extractall()\n\n\n\nfrom sklearn.model_selection import train_test_split\n\ntrain, valid = train_test_split(labels, \n                                test_size=0.1, \n                                stratify=labels['has_cactus'],\n                                random_state=50)\n\n\n\n\n#데이터셋 정의하기 495~\nimport cv2\nfrom torch.utils.data import Dataset\n\nclass ImageDataset(Dataset):\n    # 초기화 메소드\n    def __init__(self, df, img_dir='./', transform=None):\n        super().__init__()\n        self.df = df\n        self.img_dir = img_dir\n        self.transform = transform\n\n    # 데이터셋 크기 반환 메소드\n    def __len__(self):\n        return len(self.df)\n\n    # 인덱스에 해당하는 데이터 반환 메소드\n    def __getitem__(self, idx):\n        img_id = self.df.iloc[idx, 0]\n        img_path = self.img_dir + img_id\n        image=cv2.imread(img_path)\n        image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        label = self.df.iloc[idx,1]\n\n\n        if self.transform is not None:\n            image = self.transform(image)\n            return image, label\n\n# 498\nfrom torchvision import transforms\ntransform = transforms.ToTensor()\n\ndataset_train = ImageDataset(df = train, img_dir=\"train/\", transform=transform)\ndataset_valid = ImageDataset(df = valid, img_dir=\"train/\", transform=transform)\n\ndataset_train.__len__()\ndataset_valid.__len__()\n\n\n\n\n# 데이터 로더 만들기 499\n\nfrom torch.utils.data import DataLoader\nloader_train = DataLoader(dataset = dataset_train, batch_size=32, shuffle=True)\nloader_valid = DataLoader(dataset = dataset_valid, batch_size=32, shuffle=False)\n\n\n\n# 모델 만들기 501\nimport torch.nn as nn # 신경망 모듈 사용하기\nimport torch.nn.functional as F # 함수. 대문자로 표시하겠습니다.\n\n# class\nclass Model(nn.Module): # 신경망 모델\n    # 초기화\n    def __init__(self):\n        super().__init__() # nn.Model 초기화 메소드 호출\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=2)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=2)\n        self.max_pool = nn.MaxPool2d(kernel_size=2)\n        self.avg_pool = nn.AvgPool2d(kernel_size=2)\n        self.fc = nn.Linear(in_features=64*4*4, out_features=2)\n\n    # 순전파 출력\n    def forward(self, x):\n        x = self.max_pool(F.relu(self.conv1(x)))\n        x = self.max_pool(F.relu(self.conv2(x)))\n        x = self.avg_pool(x)\n        x = x.view(-1, 64 * 4 * 4)\n        x = self.fc(x)\n        return x\n\n\n\n\nmodel = Model().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n\n\n\nepochs = 10 # 총 에폭\n# 총 에폭만큼 반복\nfor epoch in range(epochs):\n    epoch_loss = 0 # 에폭별 손실값 초기화\n    \n    # '반복 횟수'만큼 반복 \n    for images, labels in loader_train:\n        # 이미지, 레이블 데이터 미니배치를 장비에 할당 \n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # 옵티마이저 내 기울기 초기화\n        optimizer.zero_grad()\n        # 순전파 : 이미지 데이터를 신경망 모델의 입력값으로 사용해 출력값 계산\n        outputs = model(images)\n        # 손실 함수를 활용해 outputs와 labels의 손실값 계산\n        loss = criterion(outputs, labels)\n        # 현재 배치에서의 손실 추가\n        epoch_loss += loss.item() \n        # 역전파 수행\n        loss.backward()\n        # 가중치 갱신\n        optimizer.step()\n        \n    # 훈련 데이터 손실값 출력\n    print(f'에폭 [{epoch+1}/{epochs}] - 손실값: {epoch_loss/len(loader_train):.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 성능 검증하기\nfrom sklearn.metrics import roc_auc_score\n\ntrue_list = []\npreds_list= []\n\nmodel.eval()\n\nwith torch.no_grad():\n    for images, labels in loader_valid:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        outputs = model(images)\n        preds = torch.softmax(outputs.cpu(), dim=1)[:, 1]\n        true = labels.cpu()\n        preds_list.extend(preds)\n        true_list.extend(true)\n\nprint(f'검증 데이터 ROC AUC : {roc_auc_score(true_list, preds_list):.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_test = ImageDataset(df=submission, img_dir='test/', transform = transform)\nloader_test = DataLoader(dataset=dataset_test, batch_size=32, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\npreds=[]\nwith torch.no_grad():\n    for images, _ in loader_test:\n        images = images.to(device)\n        outputs = model(images)\n        preds_part = torch.softmax(outputs.cpu(), dim=1)[:, 1].tolist()\n        preds.extend(preds_part)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}